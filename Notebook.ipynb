{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de documents d'opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupe E :\n",
    "Nom - numéro d'étudiant\n",
    "\n",
    "Darnala Baptiste -\n",
    "\n",
    "Di Giovanni Thomas - 21505926\n",
    "\n",
    "Duverger Eliott -\n",
    "\n",
    "Pierre (dsl je connais pas ton nom de famille) -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "movieComments = pandas.read_csv('Data/dataset.csv', sep='\\t', header = None, encoding = \"utf8\")\n",
    "movieCommentsLabels = pandas.read_csv('Data/labels.csv', sep='\\t', header = None, encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-traitements choisis :\n",
    "\n",
    "1- Supression de caractères non ASCII\n",
    "\n",
    "2- Suppression des contractions\n",
    "\n",
    "3- Passage en minuscule\n",
    "\n",
    "4- Supression de la ponctuation\n",
    "\n",
    "5- Suppressions des stopwords\n",
    "\n",
    "6- Lemmatisation\n",
    "\n",
    "Pas de remplacement des nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "from IPython.display import display\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def cleanText(commentString):   \n",
    "    # Removing non ASCII characters\n",
    "    commentString = unicodedata.normalize('NFKD', commentString).encode(\"ascii\", \"ignore\").decode(\"utf-8\", 'ignore')\n",
    "\n",
    "    # Removing contractions\n",
    "    commentString = contractions.fix(commentString, slang = True)\n",
    "\n",
    "    # Tokenizing\n",
    "    tokenizedText = word_tokenize(commentString)\n",
    "\n",
    "    # Putting all words in lowercase\n",
    "    tokenizedText = [word.lower() for word in tokenizedText]\n",
    "\n",
    "    # Deleting ponctuations\n",
    "    tokenizedText = [word for word in tokenizedText if word.isalpha()]\n",
    "\n",
    "    # Removing stop words\n",
    "    tokenizedText = [word for word in tokenizedText if not word in stopwords.words('english')]\n",
    "    \n",
    "    # Converting numbers\n",
    "    #inflectEngine = inflect.engine()\n",
    "    #newWords = []\n",
    "    #for word in tokenizedText:\n",
    "    #    if word.isdigit():\n",
    "    #        newWords.append(inflectEngine.number_to_words(word))\n",
    "    #    else:\n",
    "    #        newWords.append(word)\n",
    "    #tokenizedText = newWords\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    commentString = [lemmatizer.lemmatize(word, pos = 'v') for word in tokenizedText]\n",
    "\n",
    "    # Turning back tokens into a string\n",
    "    commentString = \"\".join([\" \" + i for i in tokenizedText]).strip()\n",
    "    \n",
    "    return commentString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sickit learn met régulièrement à jour des versions et indique des futurs warnings\n",
    "# Ces deux lignes permettent de ne pas les afficher\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des variables d'apprentissage et des variables à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieCommentsArray = movieComments.values\n",
    "data = movieCommentsArray[:, 0] # X\n",
    "\n",
    "movieCommentsLabelsArray = movieCommentsLabels.values\n",
    "dataLabels = movieCommentsLabelsArray[:, 0] # Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorisation avec TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = cleanText(data[i])\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1, 2), min_df = 0.05)\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "\n",
    "data = vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Découpage des données en jeu d'apprentissage (70%) et jeu de test (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSize = 0.7\n",
    "testingSize = 0.3\n",
    "\n",
    "trainingData, testingData, trainingDataLabels, testingDataLabels = train_test_split(data, dataLabels, train_size = trainingSize, test_size = testingSize)\n",
    "# X_train,    X_test,      Y_train,            Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifieurs SVC et Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append((\"SVC\", SVC()))\n",
    "models.append((\"Random forest\", RandomForestClassifier()))\n",
    "\n",
    "for name, model in models:\n",
    "    kFold = KFold(n_splits = 10, random_state = 10)\n",
    "    crossVal = cross_val_score(model, data, dataLabels, cv = kFold, scoring = \"accuracy\")\n",
    "    print(name, \": \", crossVal.mean(), \" (\", crossVal.std(), \") \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des classifieurs et leurs paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'SVC': [\n",
    "        {'C': [0.001, 0.01, 0.1, 1, 10]},\n",
    "        {'gamma': [0.001, 0.01, 0.1, 1]},\n",
    "        {'kernel': ['linear', 'rbf']}\n",
    "    ],\n",
    "    \n",
    "    'RandomForestClassifier': [\n",
    "        #TODO: ajouter les paramètres\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise un grid search pour rechercher le meilleur classifieur entre SVC et Random Forest, et ses meilleurs paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, classifier, parameters, score):\n",
    "        self.classifier = classifier\n",
    "        self.parameters = parameters\n",
    "        self.score = score\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.classifier, self.parameters, self.score))\n",
    "\n",
    "results = []\n",
    "for key, value in classifiers.items():\n",
    "    gridSearch = GridSearchCV(\n",
    "        estimator = value,\n",
    "        param_grid = parameters[key],\n",
    "        scoring = \"accuracy\",\n",
    "        cv = 5,\n",
    "        n_jobs = -1,\n",
    "        iid = True)\n",
    "\n",
    "    gridSearch.fit(trainingData, trainingDataLabels)\n",
    "\n",
    "    result = Model(key, gridSearch.best_score_, gridSearch.best_estimator_)\n",
    "    results.append(result)\n",
    "\n",
    "results = sorted(results, key = lambda result: result.score, reverse = True)\n",
    "\n",
    "print(\"Results from best to worst: \\n\")\n",
    "for result in results:\n",
    "    print (\"Classifier: \", result.parameters,\n",
    "    \" with score %0.2f \" %result.score, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation d'une pipeline pour sauvegarder le meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([(\"vectorizer\", TfidfVectorizer(preprocessor = cleanText, ngram_range = (1, 2), min_df = 0.05)),\n",
    "    (\"classifier\", classifiers[results[0].name])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde du modèle dans un fichier pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(results[0], open(\"groupeE.pkl\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
