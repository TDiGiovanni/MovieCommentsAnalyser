{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de documents d'opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupe E :\n",
    "Nom - numéro d'étudiant\n",
    "\n",
    "Darnala Baptiste - 21503877\n",
    "\n",
    "Di Giovanni Thomas - 21505926\n",
    "\n",
    "Duverger Eliott - 20140442\n",
    "\n",
    "Pierre Van Iseghem - 20144006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "movieComments = pandas.read_csv('Data/dataset.csv', sep = '\\t', header = None, encoding = \"utf8\")\n",
    "movieCommentsLabels = pandas.read_csv('Data/labels.csv', sep = '\\t', header = None, encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-traitements choisis :\n",
    "\n",
    "1- Supression de caractères non ASCII\n",
    "\n",
    "2- Suppression des contractions\n",
    "\n",
    "3- Passage en minuscule\n",
    "\n",
    "4- Supression de la ponctuation\n",
    "\n",
    "5- Suppressions des stopwords\n",
    "\n",
    "6- Remplacement des nombres\n",
    "\n",
    "7- Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "GoWords = ['not', 'nor', 'up', 'out', 'can']\n",
    "global OurStopWords\n",
    "OurStopWords = ['movie', 'popcorn']\n",
    "\n",
    "for word in stopwords.words('english'):\n",
    "    if GoWords.count(word) == 0:\n",
    "        OurStopWords.append(word)\n",
    "        \n",
    "def clean_text(commentString):   \n",
    "    # Removing non ASCII characters\n",
    "    commentString = unicodedata.normalize('NFKD', commentString).encode(\"ascii\", \"ignore\").decode(\"utf-8\", 'ignore')\n",
    "\n",
    "    # Removing contractions\n",
    "    commentString = contractions.fix(commentString, slang = True)\n",
    "\n",
    "    # Tokenizing\n",
    "    tokenizedText = word_tokenize(commentString)\n",
    "\n",
    "    # Putting all words in lowercase\n",
    "    tokenizedText = [word.lower() for word in tokenizedText]\n",
    "\n",
    "    # Deleting ponctuations\n",
    "    tokenizedText = [word for word in tokenizedText if word.isalpha()]\n",
    "\n",
    "    # Removing stop words\n",
    "    tokenizedText = [word for word in tokenizedText if not word in OurStopWords]\n",
    "    \n",
    "    # Converting numbers\n",
    "    inflectEngine = inflect.engine()\n",
    "    newWords = []\n",
    "    for word in tokenizedText:\n",
    "        if word.isdigit():\n",
    "            newWords.append(inflectEngine.number_to_words(word))\n",
    "        else:\n",
    "            newWords.append(word)\n",
    "    tokenizedText = newWords\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    commentString = [lemmatizer.lemmatize(word, pos = 'v') for word in tokenizedText]\n",
    "\n",
    "    # Turning back tokens into a string\n",
    "    commentString = \"\".join([\" \" + i for i in tokenizedText]).strip()\n",
    "    \n",
    "    return commentString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sickit learn met régulièrement à jour des versions et indique des futurs warnings\n",
    "# Ces deux lignes permettent de ne pas les afficher\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des variables d'apprentissage et des variables à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieCommentsArray = movieComments.values\n",
    "data = movieCommentsArray[:, 0] # X\n",
    "\n",
    "movieCommentsLabelsArray = movieCommentsLabels.values\n",
    "dataLabels = movieCommentsLabelsArray[:, 0] # Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorisation avec TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(preprocessor = cleanText, ngram_range = (1, 3), min_df = 0.01, max_df = 0.9, sublinear_tf = False, smooth_idf = True)\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "\n",
    "data = vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Découpage des données en jeu d'apprentissage (70%) et jeu de test (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSize = 0.7\n",
    "testingSize = 0.3\n",
    "\n",
    "trainingData, testingData, trainingDataLabels, testingDataLabels = train_test_split(data, dataLabels, train_size = trainingSize, test_size = testingSize)\n",
    "# X_train,    X_test,      Y_train,            Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sans grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifieurs SVC et Random forest, avec leurs paramètres par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append((\"SVC\", SVC()))\n",
    "models.append((\"Random forest\", RandomForestClassifier()))\n",
    "\n",
    "for name, model in models:\n",
    "    kFold = KFold(n_splits = 20, shuffle = True, random_state = 10)\n",
    "    crossVal = cross_val_score(model, data, dataLabels, cv = kFold, scoring = \"accuracy\")\n",
    "    print(name, \": \", crossVal.mean(), \" (\", crossVal.std(), \") \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avec grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des classifieurs et leurs paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'RandomForestClassifier': [\n",
    "        #TODO: ajouter les paramètres à tester\n",
    "    ],\n",
    "    \n",
    "    'SVC': [\n",
    "        {'C': [ 1, 2]},\n",
    "        {'kernel': ['linear']},\n",
    "        {'degree': [3]},\n",
    "        {'class_weight': ['balanced']},\n",
    "        {'probability' : ['True', 'False']},\n",
    "        {'decision_function_shape': ['ovo', 'ovr']},\n",
    "        {'random_state': [0, 1 , 5, 10]}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche du meilleur classifieur entre SVC et Random Forest, et de ses meilleurs paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, classifier, parameters, score):\n",
    "        self.classifier = classifier\n",
    "        self.parameters = parameters\n",
    "        self.score = score\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.classifier, self.parameters, self.score))\n",
    "\n",
    "results = []\n",
    "for key, value in classifiers.items():\n",
    "    gridSearch = GridSearchCV(\n",
    "        estimator = value,\n",
    "        param_grid = parameters[key],\n",
    "        scoring = \"accuracy\",\n",
    "        cv = 5,\n",
    "        n_jobs = -1,\n",
    "        iid = True)\n",
    "\n",
    "    gridSearch.fit(trainingData, trainingDataLabels)\n",
    "\n",
    "    result = Model(key, gridSearch.best_score_, gridSearch.best_estimator_)\n",
    "    results.append(result)\n",
    "\n",
    "results = sorted(results, key = lambda result: result.score, reverse = True)\n",
    "\n",
    "print(\"Results from best to worst: \\n\")\n",
    "for result in results:\n",
    "    print (\"Classifier: \", result.parameters,\n",
    "    \" with score %0.2f \" %result.score, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation d'une pipeline pour sauvegarder le meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "classifier = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                 decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "                 kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
    "                 shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", classifier)\n",
    "])\n",
    "\n",
    "pipeline.fit(trainingData, trainingDataLabels)\n",
    "\n",
    "result = pipeline.predict(testingData)\n",
    "print('\\nAccuracy: ', accuracy_score(result, testingDataLabels),'\\n')\n",
    "\n",
    "matrix = confusion_matrix(testingDataLabels, result)\n",
    "print ('\\nMatrice de confusion: \\n', matrix, \"\\n\")\n",
    "\n",
    "print ('\\n', classification_report(testingDataLabels, result), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde dans un fichier pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(pipeline, open('groupeE.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats du challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf_loaded = pickle.load(open('groupeE.pkl', 'rb'))\n",
    "\n",
    "movieComments = pandas.read_csv('Data/test_data.csv', sep = '\\t', header = None, encoding = \"utf8\")\n",
    "movieCommentsLabels = pandas.read_csv('Data/test_labels.csv', sep = '\\t', header = None, encoding = \"utf8\")\n",
    "\n",
    "movieCommentsArray = movieComments.values\n",
    "data = movieCommentsArray[:, 0] # X\n",
    "\n",
    "movieCommentsLabelsArray = movieCommentsLabels.values\n",
    "dataLabels = movieCommentsLabelsArray[:, 0] # Y\n",
    "\n",
    "result = clf_loaded.predict(data)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(result, dataLabels),'\\n')\n",
    "\n",
    "matrix = confusion_matrix(movieCommentsLabelsArray[:, 0], result)\n",
    "print('\\nMatrice de confusion: \\n', matrix, \"\\n\")\n",
    "\n",
    "print('\\n', classification_report(dataLabels, result), \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
